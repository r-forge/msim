\documentclass{article}
\usepackage{indentfirst}
\usepackage{natbib}
\usepackage{url}
\usepackage{pdfpages}  
\usepackage[fleqn]{amsmath}    % need for subequations
\usepackage{graphicx}   % need for figures
\usepackage{verbatim}   % useful for program listings
\usepackage{color}      % use if color is used in text
\usepackage{subfigure}  % use for side-by-side figures
\usepackage{hyperref}   % use for hypertext links, including those to external documents and URLs
\usepackage[normalem]{ulem}
\usepackage{amsfonts}
\usepackage{fullpage}   
\usepackage{natbib}  
\usepackage[small,compact]{titlesec} 
\begin{document}

%\VignetteIndexEntry{msim_vignette}
%\VignetteEngine{knitr} 
%\VignettePackage{msim}
\begin{center}
\Large
{\tt msim} Package Vignette
\normalsize
\end{center}

<<global,echo=FALSE,cache=FALSE>>=
library(knitr)
opts_chunk$set(warning=FALSE,message=FALSE,size="footnotesize",cache=TRUE,autodep=TRUE)
@

\section{Background}
A major difficulty in making inference about general linear mixed models (GLMMs) has been computational. To overcome numerical difficulties, alternative methods for inference have been proposed.  The first is the use of Laplace approximation which is utilized within the R package {\tt lme4}.  The second is a form of likelihood evaluation which may be in the form of Monte Carlo, numerical integral approximation, or variational approximations.  \cite{jiang} proposed the method of simulated moments (MSM)  which will be the focus of the package.  \\

MSM is methodology which is computationally feasible and consistent.  In applying the usual method of moments, one first identifies a set of sufficient statistics. A set of estimating equations is obtained by equating sample moments of the sufficient statistics to their expectations. Such expectations typically involve integrals, the highest dimension of which equals the number of sources of random effects.  Expectations are then simulated.  Finally, parameters are estimated by an appropriate optimization algorithm to solve the nonlinear system of equations.\\

This limited scope package has implemented the MSM for a logistic mixed model.  In the future, it may be expanded to include other exponential families as well as more capabilities for users.\\

\section{Statistical Setup}

Let $Y_{ij}$ be a Bernoulli response with $logit(P(y_{ij}=1)|\xi_1,...,\xi_m)=\mu+ \sigma\xi_i$, for $i=1,...,m$ independent subjects, with $j=1,..,n$ (possibly correlated) measurements per subject.   This implies that $Y_{ij}=1$ with probability $\frac{\exp(\mu+\sigma\xi_i)}{1+\exp(\mu+\sigma\xi_i)}$.\\

Let $h_{\mu,\sigma}(x)=\frac{\exp(\mu+\sigma x)}{1+\exp(\mu+\sigma x)}$ and $Y_{i\cdot}=\sum_{j=1}^n{Y_{ij}}$.  Then we see that $E(Y_{i\cdot})=nh_{\mu,\sigma}(\xi)$ and  $E(Y_{i\cdot}^2)=nh_{\mu,\sigma}(\xi)+n(n-1)h_\theta^2(\xi)$.\\

Thus, the system of equations becomes:
\begin{align*}
\frac{1}{mn}\sum_{i=1}^m y_{i\cdot}=E(h_\theta(\xi))\\
\frac{1}{(mn)(n-1)}\sum_{i=1}^m (y_{i\cdot}^2-y_{i\cdot})=E(h_\theta^2(\xi))
\end{align*}

Now, we generate $\xi_i \sim N(0,1)$ for $i=1,...,K$ and use these to generate estimates for the right sides of the system of equations. 

\begin{align*}
\frac{1}{mn}\sum_{i=1}^m y_{i\cdot}=\frac{1}{K}\sum_{i=1}^K h_{\mu,\sigma}(\xi_i)\\
\frac{1}{(mn)(m-1)}\sum_{i=1}^m (y_{i\cdot}^2-y_{i\cdot})=\frac{1}{K}\sum_{i=1}^K h_{\mu,\sigma}^2(\xi_i)
\end{align*}

\newpage\section{Reproducing Results}

We first load the {\tt msim} package in the following manner:
<<start>>=
library(msim)
@

The next thing we will try to do is replicate the results of table 1 in Jiang (1998).  His table is as follows:\\
\begin{tabular}{c c c c c}
$m$ & $n$ &True parameter &Average of estimators & SE estimators\\
\hline
20& 2& $\mu=0.2$& 0.305& 0.516\\
20& 2& $\sigma^2=1.0$& 2.895& 3.421\\
20& 6& $\mu=0.2$& 0.240& 0.302\\
20& 6& $\sigma^2=1.0$& 1.124& 0.836\\
80& 2& $\mu=0.2$& 0.180& 0.216\\
80& 2& $\sigma^2=1.0$& 1.077& 0.831\\
80& 6& $\mu=0.2$& 0.184 & 0.135\\
80& 6& $\sigma^2=1.0$& 1.031& 0.340\\\\
\end{tabular}

Several assumptions were made to do the replication since minimal information was present in the article.  First, the starting values used by Jiang were not available.  For this document, we utilized starting values very near the actual values.  Also, the random normals values used were not documented, so the random normals generated using the seed 1313 were used for each of the samples.\\  

In order to use simulate the data for the examples, we utilized the {\tt sim.data.fun} function.  We can then extract the sums by subject. 

<<simdata>>=
#Simulation 1
ys1<-sim.data.fun(m=20,n=rep(2,20),true.mu=0.2,true.sigma=1,set.seed=1313)
y.i.test1<-ys1$y.i
#Simulation 2
ys2<-sim.data.fun(m=20,n=rep(6,20),true.mu=0.2,true.sigma=1,set.seed=1313)
y.i.test2<-ys2$y.i
#Simulation 3
ys3<-sim.data.fun(m=80,n=rep(2,80),true.mu=0.2,true.sigma=1,set.seed=1313)
y.i.test3<-ys3$y.i
#Simulation 4
ys4<-sim.data.fun(m=80,n=rep(6,80),true.mu=0.2,true.sigma=1,set.seed=1313)
y.i.test4<-ys4$y.i
@

The data will be generated in the way the functions in this package will be using it.  Let's take a look at the output of the {\tt sim.data.fun}.  If one wishes to enter real data, it must be formatted like the \$y object. 

<<datasetup>>=
ys1
@

Now lets actually run the {\tt msm} function to produce the estimates for each of the simulations.  Notice that the arguments $m$ and $n$ for the the {\tt msm} function must match that of the data.\\

<<sims,autodep=TRUE>>=
#Simulation 1
estimates1<-msm(family="binomial",nsim=1000,K=1000, m=20,n=rep(2,20),
               y.i= y.i.test1,start=c(.25,1.2), true.mu =0.2,true.sigma=1)

#Simulation 2
estimates2<-msm(family="binomial",nsim=1000,K=1000, m=20,n=rep(6,20),
               y.i= y.i.test2,start=c(.25,1.2), true.mu =0.2,true.sigma=1)

#Simulation 3
estimates3<-msm(family="binomial",nsim=1000,K=1000, m=80,n=rep(2,80),
               y.i= y.i.test3,start=c(.25,1.2), true.mu =0.2,true.sigma=1)

#Simulation 4
estimates4<-msm(family="binomial",nsim=1000,K=1000, m=80,n=rep(6,80),
               y.i= y.i.test4,start=c(.25,1.2), true.mu =0.2,true.sigma=1)

table<-round(rbind(c(estimates1$mu,estimates1$mu.se),
c(estimates1$sigma2,estimates1$sigma2.se),
c(estimates2$mu,estimates2$mu.se),
c(estimates2$sigma2,estimates2$sigma2.se),
c(estimates3$mu,estimates3$mu.se),
c(estimates3$sigma2,estimates3$sigma2.se),
c(estimates4$mu,estimates4$mu.se),
c(estimates4$sigma2,estimates4$sigma2.se)),3)
rownames(table)<-c("1_mu","1_sigma2","2_mu","2_sigma2","3_mu","3_sigma2","4_mu","4_sigma2")
colnames(table)<-c("Estimate","SE")
table
@
It is clear that the numbers I produced were fairly different from the author, which may be due to shortfalls of the methodology or other undocumented differences between the author's setup and the setup used here.\\

We can also try out the methodology without giving it true values of the parameters.  This will change the methodology used for producing standard errors.  They will now be heavily based on the number of simulations done.  This is something to be improved in future versions of this package and should not be consider reliable at this point.  We will also see the estimates change slightly since new sets of random normals were generated during the estimation process.
<<notrues,autodep=TRUE>>=
#Simulation 1
est.notrue1<-msm(family="binomial",nsim=1000,K=1000, m=20,n=rep(2,20),
               y.i= y.i.test1,start=c(.25,1.2))

#Simulation 2
est.notrue2<-msm(family="binomial",nsim=1000,K=1000, m=20,n=rep(6,20),
               y.i= y.i.test2,start=c(.25,1.2))

#Simulation 3
est.notrue3<-msm(family="binomial",nsim=1000,K=1000, m=80,n=rep(2,80),
               y.i= y.i.test3,start=c(.25,1.2))

#Simulation 4
est.notrue4<-msm(family="binomial",nsim=1000,K=1000, m=80,n=rep(6,80),
               y.i= y.i.test4,start=c(.25,1.2))

table<-round(rbind(c(est.notrue1$mu,est.notrue1$mu.se),
c(est.notrue1$sigma2,est.notrue1$sigma2.se),
c(est.notrue2$mu,est.notrue2$mu.se),
c(est.notrue2$sigma2,est.notrue2$sigma2.se),
c(est.notrue3$mu,est.notrue3$mu.se),
c(est.notrue3$sigma2,est.notrue3$sigma2.se),
c(est.notrue4$mu,est.notrue4$mu.se),
c(est.notrue4$sigma2,est.notrue4$sigma2.se)),3)
rownames(table)<-c("1_mu","1_sigma2","2_mu","2_sigma2","3_mu","3_sigma2","4_mu","4_sigma2")
colnames(table)<-c("Estimate","SE")
table
@
\newpage\section{Bootstrap Correction}

In order to try to eliminate some of the bias in the estimates, a bootstrap bias correction function is implemented in the package.  Currently, this is quite computationally intensive and it is not recommended to do a high number.  Doing more bootstrap samples may not necessarily improve estimates.  However, even doing a few seemed to improve some of the estimates for the replicated example.  Again, notice that the arguments $m$ and $n$ for the the {\tt boot.msm} function must match that of the data.  The implementation is as follows:

<<boots,autodep=TRUE>>=
#Simulation 1
boot.est1<-boot.msm(msm.est =estimates1,boot.message=F,boot.sim=10,nsim=1000,K=1000, m=20,n=rep(2,20),start=c(.25,1.2))
#Simulation 2
boot.est2<-boot.msm(msm.est =estimates2,boot.message=F,boot.sim=10,nsim=1000,K=1000, m=20,n=rep(6,20),start=c(.25,1.2))
#Simulation 3
boot.est3<-boot.msm(msm.est =estimates3,boot.message=F,boot.sim=10,nsim=1000,K=1000, m=80,n=rep(2,80),start=c(.25,1.2))
#Simulation 4
boot.est4<-boot.msm(msm.est =estimates4,boot.message=F,boot.sim=10,nsim=1000,K=1000, m=80,n=rep(6,80),start=c(.25,1.2))
@

<<boottable,autodep=TRUE>>=
#Compiling the bootstrap information into a table
boottable<-round(rbind(c(boot.est1$boot.mu,boot.est1$boot.mu.se),
c(boot.est1$boot.sigma2,boot.est1$boot.sigma2.se),
c(boot.est2$boot.mu,boot.est2$boot.mu.se),
c(boot.est2$boot.sigma2,boot.est2$boot.sigma2.se),
c(boot.est3$boot.mu,boot.est3$boot.mu.se),
c(boot.est3$boot.sigma2,boot.est3$boot.sigma2.se),
c(boot.est4$boot.mu,boot.est4$boot.mu.se),
c(boot.est4$boot.sigma2,boot.est4$boot.sigma2.se)),3)
rownames(boottable)<-c("1_mu","1_sigma2","2_mu","2_sigma2","3_mu","3_sigma2","4_mu","4_sigma2")
colnames(boottable)<-c("Boot Estimate","Boot SE")
boottable
@

\newpage\section{Comparison to lme4}

We can compare this implementation with the popular {\tt lme4} package.  The results indicated that {\tt lme4} didn't do well in estimating parameters.

<<lme4>>=
library(lme4)
#Creating the lme4 style data set for simulation 1
lmecheck1<-data.frame(ys1$y)
lmecheck11<- cbind(1:20,lmecheck1$X1)
lmecheck12<- cbind(1:20,lmecheck1$X2)
lmecheck.final.1<-data.frame(rbind(lmecheck11, lmecheck12))
names(lmecheck.final.1)<-c("subject", "measurement")

#Creating the lme4 style data set for simulation 2
lmecheck2<-data.frame(ys2$y)
lmecheck21<- cbind(1:20,lmecheck2$X1)
lmecheck22<- cbind(1:20,lmecheck2$X2)
lmecheck.final.2<-data.frame(rbind(lmecheck21, lmecheck22))
names(lmecheck.final.2)<-c("subject", "measurement")

#Creating the lme4 style data set for simulation 3
lmecheck3<-data.frame(ys3$y)
lmecheck31<- cbind(1:80,lmecheck3$X1)
lmecheck32<- cbind(1:80,lmecheck3$X2)
lmecheck.final.3<-data.frame(rbind(lmecheck31, lmecheck32))
names(lmecheck.final.3)<-c("subject", "measurement")

#Creating the lme4 style data set for simulation 4
lmecheck4<-data.frame(ys4$y)
lmecheck41<- cbind(1:80,lmecheck4$X1)
lmecheck42<- cbind(1:80,lmecheck4$X2)
lmecheck.final.4<-data.frame(rbind(lmecheck41, lmecheck42))
names(lmecheck.final.4)<-c("subject", "measurement")

#Running the glmer model for each of the data sets
fm1 <- glmer(measurement ~ (1 | subject), lmecheck.final.1,family=binomial)
summary(fm1)
fm2 <- glmer(measurement ~ (1 | subject), lmecheck.final.2,family=binomial)
summary(fm2)
fm3 <- glmer(measurement ~ (1 | subject), lmecheck.final.3,family=binomial)
summary(fm3)
fm4 <- glmer(measurement ~ (1 | subject), lmecheck.final.4,family=binomial)
summary(fm4)
@

\newpage\section{Asymptotic Properties}

Let's take a look at how MSM does when we give it a lot more observations per subject.

<<sims.more>>=
#Simulating the data for 80 subjects with 20 obs each
ys.more<-sim.data.fun(m=80,n=rep(20,80),true.mu=0.2,true.sigma=1,set.seed=1313)
y.i.more1<-ys.more$y.i
estimates.more.1<-msm(family="binomial",nsim=1000,K=1000, m=80,n=rep(20,80),
               y.i= y.i.more1,start=c(.25,1.2), true.mu =0.2,true.sigma=1)

#Simulating the data for 80 subjects with 40 obs each
ys.more.2<-sim.data.fun(m=80,n=rep(40,80),true.mu=0.2,true.sigma=1,set.seed=1313)
y.i.more2<-ys.more.2$y.i
estimates.more.2<-msm(family="binomial",nsim=1000,K=1000, m=80,n=rep(40,80),
               y.i= y.i.more2,start=c(.25,1.2), true.mu =0.2,true.sigma=1)

#Simulating the data for 200 subjects with 75 obs each
ys.more.3<-sim.data.fun(m=200,n=rep(75,200),true.mu=0.2,true.sigma=1,set.seed=1313)
y.i.more3<-ys.more.3$y.i
estimates.more.3<-msm(family="binomial",nsim=1000,K=1000, m=200,n=rep(75,200),
               y.i= y.i.more3,start=c(.25,1.2), true.mu =0.2,true.sigma=1)

#Simulating the data for 200 subjects with 100 obs each
ys.more.4<-sim.data.fun(m=200,n=rep(100,200),true.mu=0.2,true.sigma=1,set.seed=1313)
y.i.more4<-ys.more.4$y.i
estimates.more.4<-msm(family="binomial",nsim=1000,K=1000, m=200,n=rep(100,200),
               y.i= y.i.more4,start=c(.25,1.2), true.mu =0.2,true.sigma=1)

table<-round(rbind(c(estimates.more.1$mu,estimates.more.1$mu.se),
c(estimates.more.1$sigma2,estimates.more.1$sigma2.se),
c(estimates.more.2$mu,estimates.more.2$mu.se),
c(estimates.more.2$sigma2,estimates.more.2$sigma2.se),
c(estimates.more.3$mu,estimates.more.3$mu.se),
c(estimates.more.3$sigma2,estimates.more.3$sigma3.se),
c(estimates.more.4$mu,estimates.more.4$mu.se),
c(estimates.more.4$sigma2,estimates.more.4$sigma2.se)),3)
rownames(table)<-c("1_mu","1_sigma2","2_mu","2_sigma2","3_mu","3_sigma2","4_mu","4_sigma2")
colnames(table)<-c("Estimate","SE")
table
@

We can also implement the bootstrapping correction in the following manner.
A lot of observations in combination with the bootstrapping bias correction seem to give much better estimates overall.  This gives us some confirmation that there is at least some asymptotic correctness to what this method is doing.
<<bootsmore,autodep=TRUE>>=
#80 subjects with 20 obs each
boot.more1<-boot.msm(msm.est =estimates.more.1,boot.message=F,boot.sim=10,nsim=1000,K=1000, m=80,n=rep(20,80),start=c(.25,1.2))
#80 subjects with 40 obs each
boot.more2<-boot.msm(msm.est =estimates.more.2,boot.message=F,boot.sim=10,nsim=1000,K=1000, m=80,n=rep(40,80),start=c(.25,1.2))
#200 subjects with 75 obs each
boot.more3<-boot.msm(msm.est =estimates.more.3,boot.message=F,boot.sim=10,nsim=1000,K=1000, m=200,n=rep(75,200),start=c(.25,1.2))
#200 subjects with 100 obs each
boot.more4<-boot.msm(msm.est =estimates.more.4,boot.message=F,boot.sim=10,nsim=1000,K=1000, m=200,n=rep(100,200),start=c(.25,1.2))

bootmore<-round(rbind(c(boot.more1$boot.mu,boot.more1$boot.mu.se),
c(boot.more1$boot.sigma2,boot.more1$boot.sigma2.se),
c(boot.more2$boot.mu,boot.more2$boot.mu.se),
c(boot.more2$boot.sigma2,boot.more2$boot.sigma2.se),
c(boot.more3$boot.mu,boot.more3$boot.mu.se),
c(boot.more3$boot.sigma2,boot.more3$boot.sigma2.se),
c(boot.more4$boot.mu,boot.more4$boot.mu.se),
c(boot.more4$boot.sigma2,boot.more4$boot.sigma2.se)),3)
rownames(bootmore)<-c("1_mu","1_sigma2","2_mu","2_sigma2","3_mu","3_sigma2","4_mu","4_sigma2")
colnames(bootmore)<-c("Boot Estimate","Boot SE")
bootmore
@

\begin{thebibliography}{}

\bibitem[Jiang (1998)]{jiang}
Jiang, J. (1998).
\newblock Consistent Estimators in Generalized Linear Mixed Models.
\newblock \emph{Journal of the American Statistical Association}, \textbf{93}, 720--729.

\bibitem[Jiang and Zhang (2001)]{jiangzhang}
Jiang, J. and Zhang, W. (2001).
\newblock Robust estimation in generalized linear mixed models.
\newblock \emph{Biometrika}, \textbf{88}, 753--765.

\end{thebibliography}

\end{document}